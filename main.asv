clc; close all; clear;


% Course TTT4275 - Estimation, detection and classification

% Running this code will display all the work that was done during 
% the music classification project. At several key points the code will
% pause and display relevant figures and diagrams, relevant information
% will be printed in the console. Pressing any button will cause the
% excecution of the program to continue.





% Import dataset
filepath = 'data/GenreClassData_30s.txt';
M = readmatrix(filepath);

file = fopen(filepath,'r');
firstLine = fgetl(file);
words = textscan(firstLine,'%s');
fclose(file);

allFeatures = string(words{1}(4:end-3))';
classes = ["pop" "metal" "disco" "blues" "reggae" "classical" "rock" "hiphop" "country" "jazz"];

% Define general constants
trainingSize = 792;
testSize = 990-trainingSize;
classLoc = size(M,2)-2;
nclasses = 10;
totalFeatures = 65;
nfeatures = 4;



%% Task 1

training = genTrainingData(M,classLoc,[11 42 7 41],trainingSize);
test = genTestData(M,classLoc,[11 42 7 41],trainingSize+1);

conm = zeros(nclasses);

% Testing
k = 5;
for n = 1:size(test,2)
    class = kNNClassifier(k,test(2:end,n),training);
    conm(test(1,n)+1,class+1) = conm(test(1,n)+1,class+1) +1;
end

corrate = trace(conm)/size(test,2);
errate = 1 - corrate;

disp('Confusion matrix:');
disp(conm);
figure;
cm = confusionchart(conm,classes,'RowSummary','row-normalized');
sortClasses(cm,'descending-diagonal');
title('kNN Classifier');
disp('Error rate:');
disp(errate);

disp('Task 1 finished');
disp('press any button to continue');
pause;
close all;

%% Task 2

%Histograms

disp('Now plotting histograms for each feature');
histogramData = genTrainingData(M,classLoc,[11 42 7 41],trainingSize);

srm = histogramData(2,:)';
mf1m = histogramData(3,:)';
spm = histogramData(4,:)';
tempo = histogramData(5,:)';

pop = srm(1:80);
metal = srm(81:160);
disco = srm(161:239);
classical = srm(400:478);

subplot(2,2,1);
hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title('spectral rolloff mean');
hold off
legend;




pop = mf1m(1:80);
metal = mf1m(81:160);
disco = mf1m(161:239);
classical = mf1m(400:478);

subplot(2,2,3);
hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title('mfcc 1 mean');
hold off
legend;

pop = spm(1:80);
metal = spm(81:160);
disco = spm(161:239);
classical = spm(400:478);

subplot(2,2,2);
hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title('spectral centroid mean');
hold off
legend;


pop = tempo(1:80);
metal = tempo(81:160);
disco = tempo(161:239);
classical = tempo(400:478);

subplot(2,2,4);
hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title('tempo');
hold off
legend;


disp('press any button to continue');
pause;
close all;

% Test when removing one feature

features = [11 42 7 41];
featuresString = ["spectral rolloff mean" "mfcc 1 mean" "spectral centroid mean" "tempo"];
k = 5;

lowestErr = inf;
lowestErrIndex = 1;

for i = 1:4
    
    currentFeats = features;
    currentFeats(i) = [];
    
    training = genTrainingData(M,classLoc,currentFeats,trainingSize);
    test = genTestData(M,classLoc,currentFeats,trainingSize+1);
    
    conm = zeros(nclasses);
    for n = 1:size(test,2)
        class = kNNClassifier(k,test(2:end,n),training);
        conm(test(1,n)+1,class+1) = conm(test(1,n)+1,class+1) +1;
    end

    corrate = trace(conm)/size(test,2);
    errate = 1 - corrate;
    
    if errate < lowestErr
        lowestErr = errate;
        lowestErrIndex = i;
    end
    
    
    disp('Removed feature:');
    disp(featuresString(i));
    disp('Confusion matrix:');
    disp(conm);
    disp('Error rate:');
    disp(errate);
    cm = confusionchart(conm,classes,'RowSummary','row-normalized');
    title(featuresString(i) + " removed");
    sortClasses(cm,'descending-diagonal');

    
    disp('press any button to continue');
    pause;
    close all;
    
end

disp('Lowest error rate is:');
disp(lowestErr);
disp('Lowest error rate occured when ' + featuresString(lowestErrIndex) + ' was removed');


disp('Task 2 finished');
disp('press any button to continue');
pause;
%% Task 3

errors = zeros(1,totalFeatures-3);
errors(11-2) = inf;
errors(42-2) = inf;
errors(7-2) = inf;

bestConm = zeros(nclasses);
lowestErr = inf;


% Brute force through all features
bestFeature = allFeatures(1);

for n = 3:totalFeatures 
    
    if n == 11 || n == 42 || n == 7
        continue
    end
    
    features = [11 42 7 n];
    training = genTrainingData(M,classLoc,features,trainingSize);
    test = genTestData(M,classLoc,features,trainingSize+1);

    conm = zeros(nclasses); 
    for i = 1:size(test,2)
        class = kNNClassifier(k,test(2:end,i),training);
        conm(test(1,i)+1,class+1) = conm(test(1,i)+1,class+1) +1; 
    end
    corr = trace(conm)/size(test,2);
    errate = 1-corr;
    errors(n-2) = errate;
    
    if errate <lowestErr
        lowestErr = errate;
        bestFeature = allFeatures(n-2);
        bestConm = conm;
    end
    
end

bestFeature = split(bestFeature,'_');


disp('Best feature was ' + bestFeature(1)+ " "  + bestFeature(2));
disp('Best feature had confusion matrix:')
disp(bestConm);
disp('Best feature had error rate:');
disp(lowestErr);
disp('press any button to continue');
cm = confusionchart(bestConm,classes,'RowSummary','row-normalized');
sortClasses(cm,'descending-diagonal');
title(bestFeature(1)+ " "  + bestFeature(2) + " added");
pause;
close all;
[min,I] = min(errors);

I = I+2;

disp('Now plotting histogram for ' + bestFeature(1)+bestFeature(2));

histogramData = genTrainingData(M,classLoc,[11 42 7 I],trainingSize);


feature = histogramData(5,:)';


pop = feature(1:80);
metal = feature(81:160);
disco = feature(161:239);
classical = feature(400:478);

hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title(bestFeature(1)+ " "  + bestFeature(2));
hold off
legend;


disp('Task 3 finished');
disp('press any button to continue');
pause;
close all;

%% Task 4

features = [11 42 7 6];


featureMatrix = zeros(90,nfeatures);
iters = 1;
song = 1;

% Generate covariance and mean vectors for each class
class_data = zeros(1+nfeatures+nfeatures^2,nclasses);

for n = 1:10
    while M(song,classLoc) == n-1
        featureMatrix(iters,:) = M(song,features);
        iters = iters+1;
        song = song +1;
    end
    
    [covar,mean] = genCovAndMean(featureMatrix(1:iters,:));
    class_data(:,n) = [n-1;mean;reshape(covar,nfeatures^2,1)];
    featureMatrix = zeros(90,nfeatures);
    iters = 1;
end




% Testing
test = genTestData(M,classLoc,features,trainingSize+1);

conm = zeros(nclasses);
for n = 1:size(test,2)
    class = gaussianClassifier(class_data,test(2:end,n),nfeatures);
    conm(test(1,n)+1,class+1) = conm(test(1,n)+1,class+1) +1;
end

corrate = trace(conm)/testSize;
errate = 1-corrate;

disp('plug-in MAP classifier error rate is:');
disp(errate);

cm = confusionchart(conm,classes,'RowSummary','row-normalized');
sortClasses(cm,'descending-diagonal');
title("plug-in MAP classifier");

disp('press any button to continue');
pause;
close all;


% Test with extra feature

features = [11 42 7 6 22];
nfeatures = 5;
featureMatrix = zeros(90,nfeatures);
iters = 1;
song = 1;

% Generate covariance and mean vectors for each class
class_data = zeros(1+nfeatures+nfeatures^2,nclasses);

for n = 1:10
    while M(song,classLoc) == n-1
        featureMatrix(iters,:) = M(song,features);
        iters = iters+1;
        song = song +1;
    end
    
    [covar,mean] = genCovAndMean(featureMatrix(1:iters,:));
    class_data(:,n) = [n-1;mean;reshape(covar,nfeatures^2,1)];
    featureMatrix = zeros(90,nfeatures);
    iters = 1;
end




% Testing
test = genTestData(M,classLoc,features,trainingSize+1);

conm = zeros(nclasses);
for n = 1:size(test,2)
    class = gaussianClassifier(class_data,test(2:end,n),nfeatures);
    conm(test(1,n)+1,class+1) = conm(test(1,n)+1,class+1) +1;
end

corrate = trace(conm)/testSize;
errate = 1-corrate;

disp('plug-in MAP classifier error rate with extra feature is:');
disp(errate);

histogramData = genTrainingData(M,classLoc,[11 42 7 22],trainingSize);


feature = histogramData(5,:)';


pop = feature(1:80);
metal = feature(81:160);
disco = feature(161:239);
classical = feature(400:478);

hold on;
histogram(pop,15);
histogram(metal,15);
histogram(disco,15);
histogram(classical,15);
title('chroma stft 6 mean');
hold off
legend;



disp('press any button to continue');
pause;
close all;

cm = confusionchart(conm,classes,'RowSummary','row-normalized');
sortClasses(cm,'descending-diagonal');
title("plug-in MAP classifier");

disp('Task 4 finished');

%% Function defs

function class = kNNClassifier(k,test,training,zscore)

%     A general kNN classifier that uses the k nearest data points in the training set
%     to classify an input
%     
%     Inputs:
%             k = number of closest data points in the training set to the input
%             test = the input vector to be classified. A Nx1 vector where N is the number of features, each element is a feature
%             training = the training data set. A N+1xM where N is the number of features and M is the size of the training set.
%                        Every column represents a labeled data point where the first row is the class label,
%                        and the next N rows are the features
%             zscore = Setting this value changes the normalization from min-max to zscore
%     Outputs:
%             class = the classifiers guess at the class of the input


    if exist('zscore','var')
        for n = 2:size(training,1)
        sigma = std(training(n,:));
        mu = mean(training(n,:));
        training(n,:) = (training(n,:)-mu)/sigma;
        test(n-1) = (test(n-1) -mu)/sigma; 
        end
    else
        for n = 2:size(training,1)
        ma = max(training(n,:));
        mi = min(training(n,:));
        training(n,:) = (training(n,:)-mi)/(ma-mi);
        test(n-1) = (test(n-1) -mi)/(ma-mi); 
        end
    end
    
    diff = pdist2(test',training(2:end,:)');
    [sorted,indexes] = sort(diff,2);
    
    kNN_labels = training(1,indexes(:,1:k));
    
    labels = mode(kNN_labels);
    class = labels(1);

    for n = 1:length(kNN_labels)
        if ismember(kNN_labels(n),labels)
            class = kNN_labels(n);
            break
        end
    end 

    
    
    
    
end


function training = genTrainingData(dataMatrix,classLoc,features,trainingSize)

%     Generates test data for classifier testing.
%     
%     Inputs: 
%             dataMatrix = Matrix containig data where each column is a feature
%                          and each row is a song.
%              classLoc = location of column containing class labels in dataMatrix
%              features = a 1XN vector containg the location of which features extract from the dataMatrix,
%                          N is the number of features.
%              trainingSize = Index of row where trainingdata ends in dataMatrix
%     Outputs:
%              training = A (N+1)xM matrix where each column represents a track and each row
%                         represents its features. The class label is contained in the first row.
%                         N is the number of features and M is the number
%                         of tracks in the training set.
%  
%                       
%     
    nFeatures = size(features,2);
    training = zeros(nFeatures+1,trainingSize);
    training(1,:) = dataMatrix(1:trainingSize,classLoc)';
    
    for n = 2:nFeatures+1
        training(n,:) = dataMatrix(1:trainingSize,features(n-1))';
    end



end




function test = genTestData(dataMatrix,classLoc,features,testStart)

%     Generates test data for classifier testing.
%     
%     Inputs: 
%             dataMatrix = Matrix containig data where each column is a feature
%                          and each row is a song.
%              classLoc = location of column containing class labels in dataMatrix
%              features = a 1XN vector containg the location of which features extract from the dataMatrix,
%                          N is the number of features.
%              testStart = Index of row where testdata starts in dataMatrix
%     Outputs:
%              test = A (N+1)xM matrix where each column represents a track and each row
%                     represents its features. The class label is contained in the first row.
%                     N is the number of features and M is the number of
%                     tracks in the test set.
%     

    nFeatures = size(features,2);
    test = zeros(nFeatures+1,size(dataMatrix,1)-testStart+1);
    test(1,:) = dataMatrix(testStart:end,classLoc)';
    
    for n = 2:nFeatures+1
        test(n,:) = dataMatrix(testStart:end,features(n-1))';
    end
end


function [covar,mean_v] = genCovAndMean(A)

%     Generates the covariance matrix of the input A, and a
%     mean vector where each element is the mean of the corresponding 
%     column of A.

    covar = cov(A);
    mean_v = zeros(size(A,2),1);
    for n = 1:size(A,2)
        mean_v(n) = mean(A(:,n));
    end

end


function class = gaussianClassifier(class_data,test,nfeatures)

%     A general plug-in MAP classifier using a gaussian single mixture model.
%     
%     Inputs:
%             class_data = A matrix where each column includes the mean vector and reshaped
%                          covariance matrix for a single class. The matrix is (N+1+N^2)xM, where N is the number of features
%                          and M is the number of classes. The first row is the class label,
%                          the next set of rows is the mean vector and the last rows are the reshaped covar matrix.
%             test = The input vector to be classified. A Nx1 vector where N is the number of features
%             nfeatures = The number of features to be used in classification.
%     Outputs:
%             class = the classifiers guess at the class of the input

    
    
    
    
    probs = zeros(2,size(class_data,2));
    probs(1,:) = 0:size(class_data,2)-1;
    
    for n = 1:size(class_data,2)
        mean_vec = class_data(2:nfeatures+1,n);
        covar = reshape(class_data(nfeatures+2:end,n),nfeatures,nfeatures);
        probs(2,n) = mvnpdf(test,mean_vec,covar);
    end
    
    probs = sortrows(probs',2)';
    probs = flip(probs,2);
    class = probs(1,1);
    

end

